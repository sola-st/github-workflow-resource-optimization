{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d024db-4023-4bc9-a88b-c95ef880e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from runs_collector.dataset import RunsDataSet\n",
    "from runs_analysis.resource_usage import get_tiers\n",
    "from optimization.github_optimization import (get_optimization_usage, \n",
    "                                              get_optimization_ts, \n",
    "                                              cancel_in_progress_impact, \n",
    "                                              calc_skip_impact,\n",
    "                                              calc_cache_impact,\n",
    "                                              get_cache_ts,\n",
    "                                              calc_cancel_in_progress_impact,\n",
    "                                              get_optimization_usage_avm,\n",
    "                                              get_optimization_ts_avm)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6eb2e-5178-407c-88da-723a48953e6c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158e2098-f45e-47b8-b76a-ba3f304f45c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from checkpoints\n",
      "Time taken to load the dataset: 57.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "data_set = RunsDataSet(None, None, from_checkpoint=True, checkpoint_dir=\"./\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken to load the dataset:\", round(end - start, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4a5897-bab3-4f42-87c3-bda3f50c651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = data_set.get_all_jobs()\n",
    "all_runs = data_set.get_all_runs()\n",
    "jobs_runs_time = all_jobs.groupby(\"run_id\").agg({\"up_time\": \"sum\", \"start_ts\": \"min\"}).reset_index()\n",
    "runs_with_time = all_runs.merge(jobs_runs_time, left_on=\"id\", right_on=\"run_id\")\n",
    "repos_list_1, repos_list_2 = get_tiers(data_set)\n",
    "all_repos = data_set.get_all_repositories()\n",
    "# paid tier repos\n",
    "list_1_names = all_repos[all_repos.id.isin(repos_list_1)].full_name.to_list()\n",
    "# free tier repos\n",
    "list_2_names = all_repos[all_repos.id.isin(repos_list_2)].full_name.to_list()\n",
    "optimizations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4063f12-0df4-4dcc-9c20-fadbdfd90d3b",
   "metadata": {},
   "source": [
    "## cancel in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36a4ba82-2e16-4ea2-843d-9dc6e8788c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_commits = []\n",
    "with open(\"collected_commits_save_poins.json\") as ccs:\n",
    "    collected_commits += json.load(ccs)\n",
    "with open(\"collected_commits_save_point_part2.json\") as ccsp:\n",
    "    collected_commits += json.load(ccsp)\n",
    "#with open(\"collected_commits_save_point_part3.json\") as ccsp:\n",
    "#    collected_commits += json.load(ccsp)\n",
    "with open(\"collected_commits_save_point_part4.json\") as ccsp:\n",
    "    collected_commits += json.load(ccsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b3894ae-571a-46a2-adc7-b056776f3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_usage = get_optimization_usage(collected_commits, optimization=\"cancel-in-progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b060b-fc55-40e6-b7bf-a8b8bcade2a0",
   "metadata": {},
   "source": [
    "### Impacted repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9482a4cc-21b6-43d6-93b6-d9b835e1f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "created_with = set([x[0] for x in cancel_usage[\"created_with_optimization\"]])\n",
    "added = set([x[0] for x in cancel_usage[\"optimization_removed\"]])\n",
    "removed = set([x[0] for x in cancel_usage[\"optimization_added\"]])\n",
    "cip_adoption_1 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_1_names))/len(list_1_names)*100\n",
    "cip_adoption_2 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_2_names))/len(list_2_names)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64ed98-181b-46ba-b24d-9d12fa668dfc",
   "metadata": {},
   "source": [
    "### Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f931d35-0b3e-4539-a15e-562ca521e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_runs1, possible_ids1, optimized_ids1 = cancel_in_progress_impact(data_set, repos_list_1, collected_commits)\n",
    "optimized_runs2, possible_ids2, optimized_ids2 = cancel_in_progress_impact(data_set, repos_list_2, collected_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157b7a9f-24b0-4280-8cfc-2bfd573ac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "cip_time2, cip_runs2, cip_cost2 = calc_cancel_in_progress_impact(data_set, possible_ids2, optimized_ids2, optimized_runs2)\n",
    "cip_time1, cip_runs1, cip_cost1 = calc_cancel_in_progress_impact(data_set, possible_ids1, optimized_ids1, optimized_runs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36097367",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"cancel_in_progress\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": cip_adoption_1,\n",
    "        \"impacted_runs\": cip_runs1,\n",
    "        \"time_impact\": cip_time1,\n",
    "        \"cost_impact\": cip_cost1\n",
    "        },\n",
    "    \"free\":{\n",
    "        \"adoption\": cip_adoption_2,\n",
    "        \"impacted_runs\": cip_runs2,\n",
    "        \"time_impact\": cip_time2,\n",
    "        \"cost_impact\": cip_cost2\n",
    "        } \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d65ed1-e8f8-4ef0-9f71-db25cdcdfd36",
   "metadata": {},
   "source": [
    "## skip workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04584a2-3f83-4d84-8f09-653fb9b57cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_dict = {}\n",
    "with open(\"commits_messages_by_repo.json\") as cmr:\n",
    "    collected_messages = json.load(cmr)\n",
    "    \n",
    "for cm in collected_messages:\n",
    "    if cm:\n",
    "        repo_name = cm[0][0]\n",
    "        if repo_name not in commits_dict:\n",
    "            commits_dict[repo_name] = [x[2] for x in cm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc86fb58-f7f3-40fe-aeb6-770df12222eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_dict_2 = {}\n",
    "with open(\"scraped_commits_messages_part2.json\") as cmr:\n",
    "    collected_messages = json.load(cmr)\n",
    "    \n",
    "for cm in collected_messages:\n",
    "    repo_name = cm[0]\n",
    "    if repo_name in commits_dict_2:\n",
    "        commits_dict_2[repo_name].append(cm[2])\n",
    "    else:\n",
    "        commits_dict_2[repo_name] = [cm[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b63e197-da65-45f3-8eb8-4f59dd558fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_dict_3 = {}\n",
    "\n",
    "with open(\"collected_commits_messages_part3.json\") as cmm:\n",
    "    collected_messages = json.load(cmm)\n",
    "    \n",
    "for cm in collected_messages:\n",
    "    repo_name = cm[0]\n",
    "    if repo_name in commits_dict_3:\n",
    "        commits_dict_3[repo_name].append(cm[2])\n",
    "    else:\n",
    "        commits_dict_3[repo_name] = [cm[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93191c95-22d6-421c-ab73-9e71247eb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_dict.update(commits_dict_2)\n",
    "commits_dict.update(commits_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60a097b5-9cc4-4c7f-ad50-e4f24e6a2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_impact_1 = calc_skip_impact(data_set, repos_list_1, commits_dict)\n",
    "skip_impact_2 = calc_skip_impact(data_set, repos_list_2, commits_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2bb73faa",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.087719298245613"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cip_adoption_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819d7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"skip_workflow\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": skip_impact_1[0],\n",
    "        \"impacted_runs\": skip_impact_1[1],\n",
    "        \"time_impact\": skip_impact_1[2],\n",
    "        \"cost_impact\": skip_impact_1[3]\n",
    "    },\n",
    "    \"free\":\n",
    "    {\n",
    "        \"adoption\": skip_impact_2[0],\n",
    "        \"impacted_runs\": skip_impact_2[1],\n",
    "        \"time_impact\": skip_impact_2[2],\n",
    "        \"cost_impact\": skip_impact_2[3]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3188f8-283f-407e-b025-140106c4a99b",
   "metadata": {},
   "source": [
    "## cache action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e807a60-357d-4f84-a5a6-25c39f1068a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pure_hashes.json\") as phj:\n",
    "    pure_hashes =  json.load(phj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a61b7d39-f729-4de7-a940-4047ebeba5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_collected_commits = []\n",
    "for commit in collected_commits:\n",
    "    new_added = []\n",
    "    new_deleted = []\n",
    "    new_modified = []\n",
    "    \n",
    "    for c in commit[\"Added\"]:\n",
    "        if c[\"commit_hash\"] in pure_hashes:\n",
    "            new_added.append(c)\n",
    "    \n",
    "    for c in commit[\"Deleted\"]:\n",
    "        if c[\"commit_hash\"] in pure_hashes:\n",
    "            new_deleted.append(c)\n",
    "    \n",
    "    for c in commit[\"Modified\"]:\n",
    "        if c[\"commit_hash\"] in pure_hashes:\n",
    "            new_modified.append(c)\n",
    "    \n",
    "    if len(new_added) + len(new_deleted) +len(new_modified) != 0:\n",
    "        new_collected_commits.append({\n",
    "            \"Added\": new_added,\n",
    "            \"Deleted\": new_deleted,\n",
    "            \"Modified\": new_modified\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf01893-5550-433c-ba29-adbb3620d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_usage = get_optimization_usage(collected_commits, optimization=\"cache@v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270bd1b9-53ff-43df-ba6b-2165f2a9bcad",
   "metadata": {},
   "source": [
    "### Impacted repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e789749-420e-4e76-ba5e-c0914b6021d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "created_with = set([x[0] for x in cache_usage[\"created_with_optimization\"]])\n",
    "added = set([x[0] for x in cache_usage[\"optimization_removed\"]])\n",
    "removed = set([x[0] for x in cache_usage[\"optimization_added\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a21e7c6-3218-4f79-9b55-537ed5fb9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption1 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_1_names)) / len(list_1_names)\n",
    "\n",
    "adoption2 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_2_names)) / len(list_2_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b8024-6f9f-4e94-bd52-ccda6efa4887",
   "metadata": {},
   "source": [
    "### Prevalence and Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba7cb5b-3f54-4ba2-a888-acfc0b202f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization = get_cache_ts(collected_commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b55b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_1, cost_1 = calc_cache_impact(new_collected_commits, data_set, repos_list_1)\n",
    "time_2, cost_2 = calc_cache_impact(new_collected_commits, data_set, repos_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "091a27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"cache\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": adoption1*100,\n",
    "        \"impacted_runs\": 100.0, # by design\n",
    "        \"time_impact\": time_1 * 100,\n",
    "        \"cost_impact\": cost_1\n",
    "    },\n",
    "    \"free\":\n",
    "    {\n",
    "        \"adoption\": adoption2*100,\n",
    "        \"impacted_runs\": 100.0,\n",
    "        \"time_impact\": time_2 * 100,\n",
    "        \"cost_impact\": cost_2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a231c67-4cba-48a0-8a7f-d2ac6dde16a8",
   "metadata": {},
   "source": [
    "## filtering target files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e3dc0e3-58c5-4257-8beb-f67960f64688",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_ignore = get_optimization_ts(collected_commits, optimization=\"paths-ignore:\")\n",
    "paths = get_optimization_ts(collected_commits, optimization=\"paths:\")\n",
    "all_repos = data_set.get_all_repositories()\n",
    "repos_1_names = all_repos[all_repos.id.isin(repos_list_1)].full_name.to_list()\n",
    "repos_2_names = all_repos[all_repos.id.isin(repos_list_2)].full_name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c0b0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_repos = data_set.get_all_repositories()\n",
    "paths_ignore = get_optimization_ts(collected_commits, optimization=\"paths-ignore:\")\n",
    "paths = get_optimization_ts(collected_commits, optimization=\"paths:\")\n",
    "paths_ignore_usage = get_optimization_ts(collected_commits, optimization=\"paths-ignore:\")\n",
    "paths_usage = get_optimization_ts(collected_commits, optimization=\"paths:\")\n",
    "cancel_inprogress = get_optimization_ts(collected_commits, optimization=\"cancel-in-progress\")\n",
    "\n",
    "#\n",
    "repos_1_names = all_repos[all_repos.id.isin(repos_list_1)].full_name.to_list()\n",
    "repos_2_names = all_repos[all_repos.id.isin(repos_list_2)].full_name.to_list()\n",
    "\n",
    "filter_adoption_1 = len(\n",
    "    set([pi[0] for pi in paths_ignore if pi[0] in repos_1_names])|set([p[0] for p in paths if p[0] in repos_1_names])\n",
    "    )/len(repos_1_names) * 100\n",
    "\n",
    "filter_adoption_2 = len(\n",
    "    set([pi[0] for pi in paths_ignore if pi[0] in repos_2_names])|set([p[0] for p in paths if p[0] in repos_2_names])\n",
    "    )/len(repos_2_names) * 100\n",
    "\n",
    "paths_ignore = [pi for pi in paths_ignore if pi[0] in repos_1_names]\n",
    "paths = [pi for pi in paths if pi[0] in repos_1_names]\n",
    "runs_repos = all_runs.merge(all_repos, left_on=\"repo_id\", right_on=\"id\")\n",
    "runs_repos[\"start_ts\"] = runs_repos.created_at.apply(lambda x: int(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").timetuple())))\n",
    "optimized_runs = []\n",
    "possible_ids = []\n",
    "for pi in paths_ignore:\n",
    "    possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "    possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "    optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "    possible_ids.extend(possible_runs.id_x.to_list())\n",
    "for pi in paths:\n",
    "    possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "    possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "    optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "    possible_ids.extend(possible_runs.id_x.to_list())\n",
    "runs_total_time = all_jobs.groupby(\"run_id\").agg({\"up_time\": \"sum\"}).reset_index()\n",
    "runs_repos = runs_repos.merge(runs_total_time, left_on=\"id_x\", right_on=\"run_id\")\n",
    "workflow_mean = runs_repos[runs_repos.workflow_id.isin(runs_repos[runs_repos.id_x.isin(optimized_runs)].workflow_id)].groupby(\"workflow_id\").agg({\"up_time\": \"mean\"}).reset_index()\n",
    "runs_repos[runs_repos.id_x.isin(optimized_runs)].\\\n",
    "merge(workflow_mean, left_on=\"workflow_id\", right_on=\"workflow_id\").up_time_y.sum()/runs_repos[runs_repos.id_x.isin(possible_ids)].up_time.sum()*100\n",
    "start_ts_min_max = runs_repos[runs_repos.id_x.isin(possible_ids)].groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"]).reset_index()\n",
    "total_start_ts = 0\n",
    "for i, row in start_ts_min_max.iterrows():\n",
    "    total_start_ts += row[\"max\"] - row[\"min\"]\n",
    "total_possible_time1 = total_start_ts/(12*30*24*3600)\n",
    "save_cost1 = runs_repos[runs_repos.id_x.isin(possible_ids)].up_time.sum()/total_possible_time1/60 * 0.008*1.52*0.005\n",
    "imp_runs_1 = len(optimized_runs)/len(possible_ids)*100\n",
    "imp_time_1 = runs_repos[runs_repos.run_id.isin(optimized_runs)].up_time.sum()/(12*30*24*3600) / total_possible_time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4c0b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_ignore = get_optimization_ts(collected_commits, optimization=\"paths-ignore:\")\n",
    "paths = get_optimization_ts(collected_commits, optimization=\"paths:\")\n",
    "paths_ignore_usage = get_optimization_ts(collected_commits, optimization=\"paths-ignore:\")\n",
    "paths_usage = get_optimization_ts(collected_commits, optimization=\"paths:\")\n",
    "cancel_inprogress = get_optimization_ts(collected_commits, optimization=\"cancel-in-progress\")\n",
    "\n",
    "repos_1_names = all_repos[all_repos.id.isin(repos_list_1)].full_name.to_list()\n",
    "repos_2_names = all_repos[all_repos.id.isin(repos_list_2)].full_name.to_list()\n",
    "\n",
    "filter_adoption_1 = len(\n",
    "    set([pi[0] for pi in paths_ignore if pi[0] in repos_1_names])|set([p[0] for p in paths if p[0] in repos_1_names])\n",
    "    )/len(repos_1_names) * 100\n",
    "\n",
    "filter_adoption_2 = len(\n",
    "    set([pi[0] for pi in paths_ignore if pi[0] in repos_2_names])|set([p[0] for p in paths if p[0] in repos_2_names])\n",
    "    )/len(repos_2_names) * 100\n",
    "\n",
    "paths_ignore = [pi for pi in paths_ignore if pi[0] in repos_2_names]\n",
    "paths = [pi for pi in paths if pi[0] in repos_2_names]\n",
    "runs_repos = all_runs.merge(all_repos, left_on=\"repo_id\", right_on=\"id\")\n",
    "runs_repos[\"start_ts\"] = runs_repos.created_at.apply(lambda x: int(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").timetuple())))\n",
    "optimized_runs = []\n",
    "possible_ids = []\n",
    "for pi in paths_ignore:\n",
    "    possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "    possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "    optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "    possible_ids.extend(possible_runs.id_x.to_list())\n",
    "for pi in paths:\n",
    "    possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "    possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "    optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "    possible_ids.extend(possible_runs.id_x.to_list())\n",
    "runs_total_time = all_jobs.groupby(\"run_id\").agg({\"up_time\": \"sum\"}).reset_index()\n",
    "runs_repos = runs_repos.merge(runs_total_time, left_on=\"id_x\", right_on=\"run_id\")\n",
    "workflow_mean = runs_repos[runs_repos.workflow_id.isin(runs_repos[runs_repos.id_x.isin(optimized_runs)].workflow_id)].groupby(\"workflow_id\").agg({\"up_time\": \"mean\"}).reset_index()\n",
    "runs_repos[runs_repos.id_x.isin(optimized_runs)].\\\n",
    "merge(workflow_mean, left_on=\"workflow_id\", right_on=\"workflow_id\").up_time_y.sum()/runs_repos[runs_repos.id_x.isin(possible_ids)].up_time.sum()*100\n",
    "start_ts_min_max = runs_repos[runs_repos.id_x.isin(possible_ids)].groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"]).reset_index()\n",
    "total_start_ts = 0\n",
    "for i, row in start_ts_min_max.iterrows():\n",
    "    total_start_ts += row[\"max\"] - row[\"min\"]\n",
    "total_possible_time2 = total_start_ts/(12*30*24*3600)\n",
    "save_cost2 = runs_repos[runs_repos.id_x.isin(possible_ids)].up_time.sum()/total_possible_time2/60 * 0.008*1.52*0.005\n",
    "imp_runs_2 = len(optimized_runs)/len(possible_ids)*100\n",
    "imp_time_2 = runs_repos[runs_repos.run_id.isin(optimized_runs)].up_time.sum()/(12*30*24*3600) / total_possible_time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9363db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"filtering_target_files\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": filter_adoption_1,\n",
    "        \"impacted_runs\": imp_runs_1,\n",
    "        \"time_impact\": imp_time_1,\n",
    "        \"cost_impact\": save_cost1\n",
    "    },\n",
    "    \"free\":\n",
    "    {\n",
    "        \"adoption\": filter_adoption_2,\n",
    "        \"impacted_runs\": imp_runs_2,\n",
    "        \"time_impact\": imp_time_1,\n",
    "        \"cost_impact\": save_cost2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188203d6-d4e4-41f2-8d10-d6a2be1131ee",
   "metadata": {},
   "source": [
    "## fail fast option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff81ebb8-543f-4e9f-b951-f163bb893ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_fast = get_optimization_usage(collected_commits, optimization=\"fail-fast:false\")\n",
    "repos_1_names = all_repos[all_repos.id.isin(repos_list_1)].full_name.to_list()\n",
    "len(set([x[0] for x in fail_fast[\"optimization_removed\"] if x[0] in repos_1_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cd46806-2e8a-41dc-a129-8dc83fd8dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_off1 = len(set([x[0] for x in fail_fast[\"created_with_optimization\"] if x[0] in list_1_names])-set([x[0] for x in fail_fast[\"optimization_removed\"] if x[0] in list_1_names]))\n",
    "len_off2 = len(set([x[0] for x in fail_fast[\"created_with_optimization\"] if x[0] in list_2_names])-set([x[0] for x in fail_fast[\"optimization_removed\"] if x[0] in list_2_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a8e6d4b-cb57-422e-b727-1eb1c9dafaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption_1 = 100 - len_off1/len(list_1_names)*100\n",
    "adoption_2 = 100 - len_off2/len(list_2_names)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "306aa4f7-b3e3-45fd-a140-9c14616b8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_fast_ts = get_optimization_ts(collected_commits, optimization=\"fail-fast:false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "700d7617-8f82-4328-922d-73ff32cfd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = data_set.get_all_runs()\n",
    "all_jobs = data_set.get_all_jobs()\n",
    "all_repos = data_set.get_all_repositories()\n",
    "runs_total_time = all_jobs.groupby(\"run_id\").agg({\"up_time\": \"sum\"}).reset_index()\n",
    "all_runs = all_runs.merge(runs_total_time, left_on=\"id\", right_on=\"run_id\")\n",
    "all_runs[\"start_ts\"] = all_runs.created_at.apply(lambda x: int(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").timetuple())))\n",
    "runs_repos = all_runs.merge(all_repos[[\"id\", \"full_name\"]], left_on=\"repo_id\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55bef7c5-5779-484b-91de-a0fda55b46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_fast_false = []\n",
    "for pi in fail_fast_ts:\n",
    "    if pi[0] in list_2_names:\n",
    "        possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "        possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "        fail_fast_false.extend(possible_runs.id_x.to_list())\n",
    "        #optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "        #possible_ids.extend(possible_runs.id_x.to_list())\n",
    "no_fail_fast_runs = all_runs[~all_runs.id.isin(fail_fast_false)]\n",
    "jobs_matrix = all_jobs[((all_jobs.name.str.contains(\"\\(\")) | (all_jobs.name.str.contains(\"matrix\")))]\n",
    "optimized_runs = no_fail_fast_runs[(no_fail_fast_runs.id.isin(jobs_matrix.run_id)) & (no_fail_fast_runs.conclusion==\"failure\")]\n",
    "no_fail_fast_success = no_fail_fast_runs[no_fail_fast_runs.conclusion==\"success\"]\n",
    "saved_time = []\n",
    "\n",
    "for i, row in optimized_runs.iterrows():\n",
    "    start_ts = row[\"start_ts\"]\n",
    "    success_df = no_fail_fast_success[(no_fail_fast_success.workflow_id == row[\"workflow_id\"])]\n",
    "    if success_df.shape[0] != 0:\n",
    "        success_time = success_df[(success_df.workflow_id == row[\"workflow_id\"])].up_time.to_list()[0]\n",
    "        saved_time.append(success_time - row[\"up_time\"])\n",
    "saved_time2 = sum(saved_time)/(no_fail_fast_runs.up_time.sum()+sum(saved_time))*100\n",
    "impacted_runs2 = optimized_runs.shape[0] / all_runs.shape[0]\n",
    "sum_start_ts = 0\n",
    "for i, row in no_fail_fast_runs.groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"]).reset_index().iterrows():\n",
    "    sum_start_ts += row[\"max\"] - row[\"min\"]\n",
    "sum_start_ts = sum_start_ts/(12*30*24*3600)\n",
    "save_cost2 = sum(saved_time)/sum_start_ts * 0.008 * 1.52 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3465e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_fast_false = []\n",
    "for pi in fail_fast_ts:\n",
    "    if pi[0] in list_1_names:\n",
    "        possible_runs = runs_repos[(runs_repos.full_name==pi[0]) & (runs_repos.workflow_file==\".github/workflows/\"+pi[1]) & (runs_repos.start_ts>pi[2]) & (runs_repos.start_ts<pi[3])]\n",
    "        possible_runs = possible_runs.sort_values(\"start_ts\")\n",
    "        fail_fast_false.extend(possible_runs.id_x.to_list())\n",
    "        #optimized_runs.extend(possible_runs[possible_runs.conclusion==\"skipped\"].id_x.to_list())\n",
    "        #possible_ids.extend(possible_runs.id_x.to_list())\n",
    "no_fail_fast_runs = all_runs[~all_runs.id.isin(fail_fast_false)]\n",
    "jobs_matrix = all_jobs[((all_jobs.name.str.contains(\"\\(\")) | (all_jobs.name.str.contains(\"matrix\")))]\n",
    "optimized_runs = no_fail_fast_runs[(no_fail_fast_runs.id.isin(jobs_matrix.run_id)) & (no_fail_fast_runs.conclusion==\"failure\")]\n",
    "no_fail_fast_success = no_fail_fast_runs[no_fail_fast_runs.conclusion==\"success\"]\n",
    "saved_time = []\n",
    "\n",
    "for i, row in optimized_runs.iterrows():\n",
    "    start_ts = row[\"start_ts\"]\n",
    "    success_df = no_fail_fast_success[(no_fail_fast_success.workflow_id == row[\"workflow_id\"])]\n",
    "    if success_df.shape[0] != 0:\n",
    "        success_time = success_df[(success_df.workflow_id == row[\"workflow_id\"])].up_time.to_list()[0]\n",
    "        saved_time.append(success_time - row[\"up_time\"])\n",
    "saved_time1 = sum(saved_time)/(no_fail_fast_runs.up_time.sum()+sum(saved_time))*100\n",
    "impacted_runs1 = optimized_runs.shape[0] / all_runs.shape[0]\n",
    "sum_start_ts = 0\n",
    "for i, row in no_fail_fast_runs.groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"]).reset_index().iterrows():\n",
    "    sum_start_ts += row[\"max\"] - row[\"min\"]\n",
    "sum_start_ts = sum_start_ts/(12*30*24*3600)\n",
    "save_cost1 = sum(saved_time)/sum_start_ts * 0.008 * 1.52 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddee72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"fail_fast\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": adoption_1,\n",
    "        \"impacted_runs\": impacted_runs1*100,\n",
    "        \"time_impact\": saved_time1,\n",
    "        \"cost_impact\": save_cost1\n",
    "    },\n",
    "    \"free\":\n",
    "    {\n",
    "        \"adoption\": adoption_2,\n",
    "        \"impacted_runs\": impacted_runs2*100,\n",
    "        \"time_impact\": saved_time2,\n",
    "        \"cost_impact\": save_cost2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb51b30-79ff-442f-a3c6-4d23183b9765",
   "metadata": {},
   "source": [
    "## Vm minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0a3261f-9faf-410e-b48a-1ff16ca6a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_minutes = get_optimization_usage_avm(collected_commits, optimization=\"timeout-minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7aff0d3a-55b0-4dca-bbd5-63eda808167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimization_ts_avm(collected_commits, optimization=\"cache@\"):\n",
    "    optimization_dict = get_optimization_usage_avm(collected_commits, optimization=optimization)\n",
    "    optimization_added = optimization_dict[\"optimization_added\"]\n",
    "    optimization_removed = optimization_dict[\"optimization_removed\"]\n",
    "    created_with_optimization = optimization_dict[\"created_with_optimization\"]\n",
    "    all_workflows = [(*tup, \"added\") for tup in optimization_added] + [(*tup, \"removed\") for tup in optimization_removed] + [(*tup, \"added\") for tup in created_with_optimization]\n",
    "    ref_temp = 1679310197\n",
    "    optimization_history = []\n",
    "    workflows_set = list(set([(ca[0], ca[1]) for ca in optimization_added]+[(cwc[0], cwc[1]) for cwc in created_with_optimization]))\n",
    "    for i in range(len(workflows_set)):\n",
    "        repo_name = workflows_set[i][0]\n",
    "        wf = workflows_set[i][1]\n",
    "        specific_workflow = [tup for tup in all_workflows if tup[0]==repo_name and tup[1]==wf]\n",
    "        specific_workflow.sort(key=lambda x: x[2])\n",
    "        optimization_seq = []\n",
    "\n",
    "        previous = specific_workflow[0][-1]\n",
    "        optimization_seq.append(specific_workflow[0])\n",
    "        for sw in specific_workflow[1:]:\n",
    "            if sw[-1] == previous:\n",
    "                if previous == \"added\":\n",
    "                    continue\n",
    "                else:\n",
    "                    optimization_seq[-1] = sw\n",
    "            else:\n",
    "                previous = sw[-1]\n",
    "                optimization_seq.append(sw)\n",
    "        \n",
    "        optimization_ts = []\n",
    "\n",
    "        if len(optimization_ts) == 1:\n",
    "            optimization_ts.append((repo_name, wf, optimization_seq[0][2], ref_temp))\n",
    "        else:\n",
    "            for i in range(len(optimization_seq)-1):\n",
    "                if optimization_seq[i][-1]==\"added\":\n",
    "                    optimization_ts.append((repo_name, wf, optimization_seq[i][2], optimization_seq[i+1][2], optimization_seq[i][3]))\n",
    "                else:\n",
    "                    continue\n",
    "            if optimization_seq[-1][-1]==\"removed\":\n",
    "                pass\n",
    "            elif optimization_seq[-1][-1]==\"added\":\n",
    "                optimization_ts.append((repo_name, wf, optimization_seq[-1][2], ref_temp, optimization_seq[-1][3]))\n",
    "            \n",
    "        if optimization_ts:\n",
    "            optimization_history.extend(optimization_ts)\n",
    "\n",
    "    return optimization_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fad7b93f-74ef-4b3a-aa38-b5142a2a4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout_ = get_optimization_ts_avm(collected_commits, optimization=\"timeout-minutes:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de45d906-2610-4c32-8af7-362b3dc719df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_repos = data_set.get_all_repositories()\n",
    "all_runs = data_set.get_all_runs()\n",
    "all_runs = all_runs[all_runs.repo_id.isin(repos_list_2)]\n",
    "all_jobs = data_set.get_all_jobs()\n",
    "\n",
    "all_runs[\"start_ts\"] = all_runs.created_at.apply(lambda x: int(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").timetuple())))\n",
    "runs_repos = all_runs.merge(all_repos, left_on=\"repo_id\", right_on=\"id\")\n",
    "\n",
    "optimized_runs_ids = []\n",
    "saved_time = []\n",
    "jobs_ids = []\n",
    "all_possible = []\n",
    "for c in timeout_:\n",
    "    candidate_runs = runs_repos[(runs_repos.full_name==c[0]) & \n",
    "            (runs_repos.workflow_file==\".github/workflows/\"+c[1]) & \n",
    "            (runs_repos.start_ts > c[2])& \n",
    "            (runs_repos.start_ts < c[3])]\n",
    "    timeout_value = c[4]\n",
    "    timed_out_jobs = all_jobs[(all_jobs.run_id.isin(candidate_runs.id_x)) & (all_jobs.up_time > timeout_value*60-59) & (all_jobs.up_time<timeout_value*60+59)]\n",
    "    max_job_time = all_jobs[all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&(runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list())].up_time.max()\n",
    "    average_high = all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))&\n",
    "                           (all_jobs.up_time>timeout_value*60+30)].up_time.mean()\n",
    "    probability_high = all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))&\n",
    "                           (all_jobs.up_time>timeout_value*60+30)].shape[0]/(all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))].shape[0]+0.1)\n",
    "    all_possible.extend(candidate_runs.id_x.to_list())\n",
    "    if not np.isnan(average_high):\n",
    "        for i, row in timed_out_jobs.iterrows():\n",
    "            saved_time.append((average_high - timeout_value*60))\n",
    "            jobs_ids.append(row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57ff06de-a2db-4477-b03c-16f78b230044",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_vm_time2 = sum(saved_time) / (all_jobs[all_jobs.run_id.isin(all_possible)].up_time.sum()+sum(saved_time)) *100\n",
    "start_ts_min_max = runs_repos[runs_repos.id_x.isin(all_possible)].groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"])\n",
    "total_start_ts = 0\n",
    "for i, row in start_ts_min_max.iterrows():\n",
    "    total_start_ts += row[\"max\"] - row[\"min\"]\n",
    "impact_cost2 = sum(saved_time) / (total_start_ts/(12*30*24*3600)) * 1.52 * 0.008 /60\n",
    "impact_runs2 =len(all_possible)/runs_repos.shape[0]*100\n",
    "#cache_usage = get_optimization_usage(collected_commits, \"concurrency:\")\n",
    "created_with = set([(x[0],x[1]) for x in cache_usage[\"created_with_optimization\"]])\n",
    "added = set([(x[0],x[1]) for x in cache_usage[\"optimization_removed\"]])\n",
    "removed = set([(x[0],x[1]) for x in cache_usage[\"optimization_added\"]])\n",
    "len((created_with|\n",
    "    added|\n",
    "    removed))\n",
    "#cache_usage = get_optimization_usage(collected_commits, \"concurrency:\")\n",
    "created_with = set([x[0] for x in vm_minutes[\"created_with_optimization\"]])\n",
    "added = set([x[0] for x in vm_minutes[\"optimization_removed\"]])\n",
    "removed = set([x[0] for x in vm_minutes[\"optimization_added\"]])\n",
    "adoption2 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_2_names)) / len(list_2_names) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6a50806",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_repos = data_set.get_all_repositories()\n",
    "all_runs = data_set.get_all_runs()\n",
    "all_runs = all_runs[all_runs.repo_id.isin(repos_list_1)]\n",
    "all_jobs = data_set.get_all_jobs()\n",
    "\n",
    "all_runs[\"start_ts\"] = all_runs.created_at.apply(lambda x: int(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%SZ\").timetuple())))\n",
    "runs_repos = all_runs.merge(all_repos, left_on=\"repo_id\", right_on=\"id\")\n",
    "\n",
    "optimized_runs_ids = []\n",
    "saved_time = []\n",
    "jobs_ids = []\n",
    "all_possible = []\n",
    "for c in timeout_:\n",
    "    candidate_runs = runs_repos[(runs_repos.full_name==c[0]) & \n",
    "            (runs_repos.workflow_file==\".github/workflows/\"+c[1]) & \n",
    "            (runs_repos.start_ts > c[2])& \n",
    "            (runs_repos.start_ts < c[3])]\n",
    "    timeout_value = c[4]\n",
    "    timed_out_jobs = all_jobs[(all_jobs.run_id.isin(candidate_runs.id_x)) & (all_jobs.up_time > timeout_value*60-59) & (all_jobs.up_time<timeout_value*60+59)]\n",
    "    max_job_time = all_jobs[all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&(runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list())].up_time.max()\n",
    "    average_high = all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))&\n",
    "                           (all_jobs.up_time>timeout_value*60+30)].up_time.mean()\n",
    "    probability_high = all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))&\n",
    "                           (all_jobs.up_time>timeout_value*60+30)].shape[0]/(all_jobs[(all_jobs.run_id.isin(runs_repos[(runs_repos.full_name==c[0])&\n",
    "                                                            (runs_repos.workflow_file==\".github/workflows/\"+c[1])].id_x.to_list()))].shape[0]+0.1)\n",
    "    all_possible.extend(candidate_runs.id_x.to_list())\n",
    "    if not np.isnan(average_high):\n",
    "        for i, row in timed_out_jobs.iterrows():\n",
    "            saved_time.append((average_high - timeout_value*60))\n",
    "            jobs_ids.append(row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1526fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_vm_time1 = sum(saved_time) / (all_jobs[all_jobs.run_id.isin(all_possible)].up_time.sum()+sum(saved_time)) *100\n",
    "start_ts_min_max = runs_repos[runs_repos.id_x.isin(all_possible)].groupby(\"repo_id\").start_ts.agg([\"min\", \"max\"])\n",
    "total_start_ts = 0\n",
    "for i, row in start_ts_min_max.iterrows():\n",
    "    total_start_ts += row[\"max\"] - row[\"min\"]\n",
    "impact_cost1 = sum(saved_time) / (total_start_ts/(12*30*24*3600)) * 1.52 * 0.008 /60\n",
    "impact_runs1 =len(all_possible)/runs_repos.shape[0]*100\n",
    "#cache_usage = get_optimization_usage(collected_commits, \"concurrency:\")\n",
    "created_with = set([(x[0],x[1]) for x in cache_usage[\"created_with_optimization\"]])\n",
    "added = set([(x[0],x[1]) for x in cache_usage[\"optimization_removed\"]])\n",
    "removed = set([(x[0],x[1]) for x in cache_usage[\"optimization_added\"]])\n",
    "len((created_with|\n",
    "    added|\n",
    "    removed))\n",
    "#cache_usage = get_optimization_usage(collected_commits, \"concurrency:\")\n",
    "created_with = set([x[0] for x in vm_minutes[\"created_with_optimization\"]])\n",
    "added = set([x[0] for x in vm_minutes[\"optimization_removed\"]])\n",
    "removed = set([x[0] for x in vm_minutes[\"optimization_added\"]])\n",
    "adoption1 = len((created_with|\n",
    "    added|\n",
    "    removed) & set(list_1_names)) / len(list_1_names) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "214bd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizations[\"custom_timeout\"]={\n",
    "    \"paid\":{\n",
    "        \"adoption\": adoption1,\n",
    "        \"impacted_runs\": impacted_runs1*100,\n",
    "        \"time_impact\": impact_vm_time1,\n",
    "        \"cost_impact\": impact_cost1\n",
    "    },\n",
    "    \"free\":\n",
    "    {\n",
    "        \"adoption\": adoption2,\n",
    "        \"impacted_runs\": impacted_runs2*100,\n",
    "        \"time_impact\": impact_vm_time2,\n",
    "        \"cost_impact\": impact_cost2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13093eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Adoption rate %          Impacted runs %          Impact on VM-time %      Annual cost delta $     \n",
      "                              ------------------------------------------------------------------------------------------------\n",
      "Conclusion                     Paid         Free         Paid         Free         Paid         Free         Paid         Free        \n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "cache                          32.9         17.8         100.0        100.0        -3.4         -6.0         -21.74       -0.6        \n",
      "fail_fast                      75.9         84.5         3.1          4.7          1.5          2.0          2.14         4.21        \n",
      "cancel_in_progress             10.1         1.9          9.2          1.7          4.2          1.6          63.29        0.53        \n",
      "skip_workflow                  10.1         4.8          0.1          0.3          0.0          0.4          2.52         0.75        \n",
      "filtering_target_files         21.1         8.7          0.1          1.8          0.0          0.0          2.27         0.06        \n",
      "custom_timeout                 14.0         2.6          3.1          4.7          8.1          12.9         58.33        1.58        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<30} {:<24} {:<24} {:<24} {:<24}\".format(\n",
    "        \"\",\n",
    "        \"Adoption rate %\",\n",
    "        \"Impacted runs %\",\n",
    "        \"Impact on VM-time %\",\n",
    "        \"Annual cost delta $\"\n",
    "         ))\n",
    "print(\" \"*30 + \"-\"*96)\n",
    "print(\"{:<30} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\".format(\n",
    "        \"Conclusion\",\n",
    "        \"Paid\",\n",
    "        \"Free\",\n",
    "        \"Paid\",\n",
    "        \"Free\",\n",
    "        \"Paid\",\n",
    "        \"Free\",\n",
    "        \"Paid\",\n",
    "        \"Free\"\n",
    "         ))\n",
    "print(\"-\"*126)\n",
    "for op_name in [\"cache\", \"fail_fast\", \"cancel_in_progress\", \"skip_workflow\", \"filtering_target_files\", \"custom_timeout\"]:\n",
    "    o = optimizations[op_name]\n",
    "    print(\"{:<30} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\".format(\n",
    "        op_name,\n",
    "        round(o[\"paid\"][\"adoption\"], 1),\n",
    "        round(o[\"free\"][\"adoption\"], 1),\n",
    "        round(o[\"paid\"][\"impacted_runs\"], 1),\n",
    "        round(o[\"free\"][\"impacted_runs\"], 1),\n",
    "        round(o[\"paid\"][\"time_impact\"], 1),\n",
    "        round(o[\"free\"][\"time_impact\"], 1),\n",
    "        round(o[\"paid\"][\"cost_impact\"], 2),\n",
    "        round(o[\"free\"][\"cost_impact\"], 2),\n",
    "         ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
